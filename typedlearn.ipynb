{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16e3db-5b32-4cec-a996-f3cfdc17f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Bootstrap (config + stable phase logging + EMNIST data-dir discovery)\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, asdict, is_dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Mapping, Optional, Tuple\n",
    "import os, sys, json, random, hashlib\n",
    "\n",
    "# ============================================================\n",
    "# Stable phase logging\n",
    "# ============================================================\n",
    "\n",
    "def _jsonable(x: Any) -> Any:\n",
    "    if isinstance(x, Path):\n",
    "        return str(x)\n",
    "    if is_dataclass(x):\n",
    "        return {k: _jsonable(v) for k, v in asdict(x).items()}\n",
    "    if isinstance(x, dict):\n",
    "        return {k: _jsonable(v) for k, v in x.items()}\n",
    "    if isinstance(x, list):\n",
    "        return [_jsonable(y) for y in x]\n",
    "    if isinstance(x, tuple):\n",
    "        return [_jsonable(y) for y in x]\n",
    "    if isinstance(x, set):\n",
    "        return sorted(_jsonable(y) for y in x)\n",
    "    return x\n",
    "\n",
    "PHASES: dict[str, dict[str, Any]] = {}\n",
    "\n",
    "def phase_print(phase: str, payload: Mapping[str, Any]) -> None:\n",
    "    if \"phase\" in payload:\n",
    "        raise KeyError(\"phase_print payload must not contain key 'phase'\")\n",
    "    PHASES[phase] = _jsonable(dict(payload))\n",
    "    obj = {\"phase\": phase, **payload}\n",
    "    obj = {k: _jsonable(v) for k, v in obj.items()}\n",
    "    print(json.dumps(obj, indent=2, sort_keys=True, ensure_ascii=False, allow_nan=False))\n",
    "\n",
    "# ============================================================\n",
    "# Run config (single source of truth for Cell 3)\n",
    "# ============================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RunConfig:\n",
    "    # provenance / dataset\n",
    "    seed: int = 0\n",
    "    run_mode: str = \"quick\"     # {\"quick\",\"full\"}\n",
    "    split: str = \"digits\"       # EMNIST split name\n",
    "\n",
    "    # semantics lock (critical)\n",
    "    canonicalize: bool = True   # upright canonicalization\n",
    "    bin_threshold: int = 127    # binarize pixel > threshold\n",
    "\n",
    "    # caps (0 means uncapped)\n",
    "    max_train: int = 20_000\n",
    "    max_test: int = 5_000\n",
    "\n",
    "    # Cell 3 (compiled discrete margin repair) knobs\n",
    "    finite_n: int = 256\n",
    "    cap_unary: int = 128\n",
    "    margin: int = 1\n",
    "    overshoot_delta: int = 2\n",
    "    max_epochs: int = 50\n",
    "    max_updates: int = 20_000\n",
    "    probe_per_class: int = 10\n",
    "\n",
    "def load_config() -> RunConfig:\n",
    "    seed = int(os.environ.get(\"SEED\", \"0\"))\n",
    "    run_mode = os.environ.get(\"RUN_MODE\", \"quick\").strip().lower()\n",
    "    if run_mode not in {\"quick\", \"full\"}:\n",
    "        raise ValueError(f\"RUN_MODE must be 'quick' or 'full', got: {run_mode!r}\")\n",
    "\n",
    "    split = os.environ.get(\"EMNIST_SPLIT\", \"digits\").strip().lower()\n",
    "\n",
    "    # semantics lock\n",
    "    canonicalize = True\n",
    "    bin_threshold = int(os.environ.get(\"BIN_THRESHOLD\", \"127\"))\n",
    "\n",
    "    # caps by run_mode (quick defaults)\n",
    "    if run_mode == \"quick\":\n",
    "        max_train = int(os.environ.get(\"MAX_TRAIN\", \"20000\"))\n",
    "        max_test  = int(os.environ.get(\"MAX_TEST\",  \"5000\"))\n",
    "    else:\n",
    "        max_train = int(os.environ.get(\"MAX_TRAIN\", \"0\"))\n",
    "        max_test  = int(os.environ.get(\"MAX_TEST\",  \"0\"))\n",
    "\n",
    "    # Cell 3 knobs (override via env if desired)\n",
    "    finite_n        = int(os.environ.get(\"FINITE_N\", \"256\"))\n",
    "    cap_unary       = int(os.environ.get(\"CAP_UNARY\", \"128\"))\n",
    "    margin          = int(os.environ.get(\"MARGIN\", \"1\"))\n",
    "    overshoot_delta = int(os.environ.get(\"OVERSHOOT_DELTA\", \"2\"))\n",
    "    max_epochs      = int(os.environ.get(\"MAX_EPOCHS\", \"50\"))\n",
    "    max_updates     = int(os.environ.get(\"MAX_UPDATES\", \"20000\"))\n",
    "    probe_per_class = int(os.environ.get(\"PROBE_PER_CLASS\", \"10\"))\n",
    "\n",
    "    return RunConfig(\n",
    "        seed=seed,\n",
    "        run_mode=run_mode,\n",
    "        split=split,\n",
    "        canonicalize=canonicalize,\n",
    "        bin_threshold=bin_threshold,\n",
    "        max_train=max_train,\n",
    "        max_test=max_test,\n",
    "        finite_n=finite_n,\n",
    "        cap_unary=cap_unary,\n",
    "        margin=margin,\n",
    "        overshoot_delta=overshoot_delta,\n",
    "        max_epochs=max_epochs,\n",
    "        max_updates=max_updates,\n",
    "        probe_per_class=probe_per_class,\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# EMNIST data directory discovery\n",
    "# ============================================================\n",
    "\n",
    "def find_data_dir(start: Optional[Path] = None) -> Tuple[Path, str]:\n",
    "    env = os.environ.get(\"EMNIST_DATA_DIR\", \"\").strip()\n",
    "    if env:\n",
    "        p = Path(env).expanduser().resolve()\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"EMNIST_DATA_DIR is set but does not exist: {p}\")\n",
    "        if not p.is_dir():\n",
    "            raise NotADirectoryError(f\"EMNIST_DATA_DIR exists but is not a directory: {p}\")\n",
    "        return p, \"env:EMNIST_DATA_DIR\"\n",
    "\n",
    "    start = (start or Path.cwd()).resolve()\n",
    "    for parent in [start] + list(start.parents):\n",
    "        cand = parent / \"gzip\"\n",
    "        if cand.exists() and cand.is_dir():\n",
    "            return cand.resolve(), \"found:gzip_ancestor\"\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Cannot locate EMNIST data directory. Expected either:\\n\"\n",
    "        \"  - EMNIST_DATA_DIR=<path-to-dir-containing-emnist-gz-files>\\n\"\n",
    "        \"  - or a ./gzip/ directory in the current working directory or any ancestor.\"\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# Instantiate run context\n",
    "# ============================================================\n",
    "\n",
    "CFG = load_config()\n",
    "random.seed(CFG.seed)\n",
    "\n",
    "DATA_DIR_START = Path.cwd().resolve()\n",
    "DATA_DIR, DATA_DIR_METHOD = find_data_dir(start=DATA_DIR_START)\n",
    "PROJECT_ROOT = DATA_DIR.parent.resolve()\n",
    "\n",
    "phase_print(\"run_header\", {\n",
    "    \"seed\": CFG.seed,\n",
    "    \"run_mode\": CFG.run_mode,\n",
    "    \"split\": CFG.split,\n",
    "    \"canonicalize\": \"upright\" if CFG.canonicalize else \"raw\",\n",
    "    \"bin_threshold\": CFG.bin_threshold,\n",
    "    \"caps\": {\"max_train\": CFG.max_train, \"max_test\": CFG.max_test},\n",
    "    \"cell3_cfg\": {\n",
    "        \"finite_n\": CFG.finite_n,\n",
    "        \"cap_unary\": CFG.cap_unary,\n",
    "        \"margin\": CFG.margin,\n",
    "        \"overshoot_delta\": CFG.overshoot_delta,\n",
    "        \"max_epochs\": CFG.max_epochs,\n",
    "        \"max_updates\": CFG.max_updates,\n",
    "        \"probe_per_class\": CFG.probe_per_class,\n",
    "    },\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"python_hash_seed\": os.environ.get(\"PYTHONHASHSEED\", None),\n",
    "    \"project_root\": PROJECT_ROOT,\n",
    "    \"data_dir\": DATA_DIR,\n",
    "    \"data_dir_method\": DATA_DIR_METHOD,\n",
    "    \"data_dir_start\": DATA_DIR_START,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27534c8-8c9e-4cb3-8f6d-268b1f520903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — EMNIST kernel (streamed IDX-gz reader + mapping + canonicalization + dataset streams)\n",
    "from __future__ import annotations\n",
    "\n",
    "import gzip, io, struct\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterator, List, Tuple, Callable\n",
    "\n",
    "# ============================================================\n",
    "# Semantics-critical constants\n",
    "# ============================================================\n",
    "\n",
    "GRID: int = 28\n",
    "N_PIXELS: int = GRID * GRID\n",
    "\n",
    "# ============================================================\n",
    "# IDX (.ubyte) reading\n",
    "# ============================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class IdxImagesHeader:\n",
    "    count: int\n",
    "    rows: int\n",
    "    cols: int\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class IdxLabelsHeader:\n",
    "    count: int\n",
    "\n",
    "def _read_exact(f: io.BufferedReader, n: int) -> bytes:\n",
    "    b = f.read(n)\n",
    "    if b is None or len(b) != n:\n",
    "        raise EOFError(f\"expected {n} bytes, got {0 if b is None else len(b)}\")\n",
    "    return b\n",
    "\n",
    "def read_idx_images_header(f: io.BufferedReader) -> IdxImagesHeader:\n",
    "    magic, count, rows, cols = struct.unpack(\">IIII\", _read_exact(f, 16))\n",
    "    if magic != 2051:\n",
    "        raise ValueError(f\"bad IDX images magic {magic} (expected 2051)\")\n",
    "    return IdxImagesHeader(count=int(count), rows=int(rows), cols=int(cols))\n",
    "\n",
    "def read_idx_labels_header(f: io.BufferedReader) -> IdxLabelsHeader:\n",
    "    magic, count = struct.unpack(\">II\", _read_exact(f, 8))\n",
    "    if magic != 2049:\n",
    "        raise ValueError(f\"bad IDX labels magic {magic} (expected 2049)\")\n",
    "    return IdxLabelsHeader(count=int(count))\n",
    "\n",
    "class EMNISTIdxGz:\n",
    "    \"\"\"\n",
    "    Stream (image_bytes, label_int) pairs from IDX ubyte gz files without loading all into memory.\n",
    "    - image_bytes: 28*28 bytes, raw grayscale 0..255\n",
    "    - label_int: original dataset label integer (before mapping)\n",
    "    \"\"\"\n",
    "    def __init__(self, images_gz: Path, labels_gz: Path) -> None:\n",
    "        self.images_gz = Path(images_gz)\n",
    "        self.labels_gz = Path(labels_gz)\n",
    "\n",
    "    def __iter__(self) -> Iterator[Tuple[bytes, int]]:\n",
    "        with gzip.open(self.images_gz, \"rb\") as fi_gz, gzip.open(self.labels_gz, \"rb\") as fl_gz:\n",
    "            fi = io.BufferedReader(fi_gz)\n",
    "            fl = io.BufferedReader(fl_gz)\n",
    "\n",
    "            hi = read_idx_images_header(fi)\n",
    "            hl = read_idx_labels_header(fl)\n",
    "\n",
    "            if hi.count != hl.count:\n",
    "                raise ValueError(f\"count mismatch: images={hi.count}, labels={hl.count}\")\n",
    "            if hi.rows != GRID or hi.cols != GRID:\n",
    "                raise ValueError(f\"expected {GRID}x{GRID} but got {hi.rows}x{hi.cols}\")\n",
    "\n",
    "            img_n = hi.rows * hi.cols\n",
    "            for _ in range(hi.count):\n",
    "                img = _read_exact(fi, img_n)\n",
    "                lab = _read_exact(fl, 1)[0]\n",
    "                yield img, int(lab)\n",
    "\n",
    "def split_files(split: str) -> Tuple[str, str, str, str, str]:\n",
    "    s = split.strip().lower()\n",
    "    mapping = f\"emnist-{s}-mapping.txt\"\n",
    "    train_images = f\"emnist-{s}-train-images-idx3-ubyte.gz\"\n",
    "    train_labels = f\"emnist-{s}-train-labels-idx1-ubyte.gz\"\n",
    "    test_images = f\"emnist-{s}-test-images-idx3-ubyte.gz\"\n",
    "    test_labels = f\"emnist-{s}-test-labels-idx1-ubyte.gz\"\n",
    "    return mapping, train_images, train_labels, test_images, test_labels\n",
    "\n",
    "# ============================================================\n",
    "# Mapping file parsing\n",
    "# ============================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ClassMapping:\n",
    "    label_to_class: Dict[int, int]\n",
    "    class_to_label: List[int]\n",
    "    class_to_codepoint: List[int]\n",
    "\n",
    "def parse_emnist_mapping_txt(path: Path) -> ClassMapping:\n",
    "    pairs: List[Tuple[int, int]] = []\n",
    "    seen_labels: set[int] = set()\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if not s or s.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = s.split()\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            lab = int(parts[0])\n",
    "            cp = int(parts[1])\n",
    "            if lab in seen_labels:\n",
    "                raise ValueError(f\"duplicate label {lab} in mapping file {path}\")\n",
    "            seen_labels.add(lab)\n",
    "            pairs.append((lab, cp))\n",
    "\n",
    "    if not pairs:\n",
    "        raise ValueError(f\"no mapping pairs parsed from {path}\")\n",
    "\n",
    "    pairs.sort(key=lambda t: t[0])\n",
    "    labels = [p[0] for p in pairs]\n",
    "    cps = [p[1] for p in pairs]\n",
    "    label_to_class: Dict[int, int] = {lab: i for i, lab in enumerate(labels)}\n",
    "    return ClassMapping(label_to_class=label_to_class, class_to_label=labels, class_to_codepoint=cps)\n",
    "\n",
    "# ============================================================\n",
    "# Semantics: canonicalization (upright = rotate CW 90 then mirror horizontally)\n",
    "# ============================================================\n",
    "\n",
    "def flip_horizontal(img: bytes, *, grid: int = GRID) -> bytes:\n",
    "    if len(img) != grid * grid:\n",
    "        raise ValueError(f\"expected image length {grid*grid}, got {len(img)}\")\n",
    "    out = bytearray(grid * grid)\n",
    "    for r in range(grid):\n",
    "        row = img[r * grid:(r + 1) * grid]\n",
    "        out[r * grid:(r + 1) * grid] = row[::-1]\n",
    "    return bytes(out)\n",
    "\n",
    "def rotate90_cw(img: bytes, *, grid: int = GRID) -> bytes:\n",
    "    if len(img) != grid * grid:\n",
    "        raise ValueError(f\"expected image length {grid*grid}, got {len(img)}\")\n",
    "    out = bytearray(grid * grid)\n",
    "    for r in range(grid):\n",
    "        for c in range(grid):\n",
    "            out[c * grid + (grid - 1 - r)] = img[r * grid + c]\n",
    "    return bytes(out)\n",
    "\n",
    "def canonicalize_upright(img: bytes, *, grid: int = GRID) -> bytes:\n",
    "    return flip_horizontal(rotate90_cw(img, grid=grid), grid=grid)\n",
    "\n",
    "# ============================================================\n",
    "# Re-iterable dataset streams for Cell 3\n",
    "# ============================================================\n",
    "\n",
    "class _IterableFromFactory:\n",
    "    def __init__(self, factory: Callable[[], Iterator[Tuple[bytes, int]]]) -> None:\n",
    "        self._factory = factory\n",
    "    def __iter__(self) -> Iterator[Tuple[bytes, int]]:\n",
    "        return self._factory()\n",
    "\n",
    "def iter_raw_images_labels(*, train: bool) -> Iterator[Tuple[bytes, int]]:\n",
    "    mapping_file, tr_i, tr_l, te_i, te_l = split_files(CFG.split)\n",
    "    d = Path(DATA_DIR)\n",
    "    mapping = parse_emnist_mapping_txt(d / mapping_file)\n",
    "\n",
    "    if train:\n",
    "        ds = EMNISTIdxGz(d / tr_i, d / tr_l)\n",
    "        cap = int(CFG.max_train)\n",
    "    else:\n",
    "        ds = EMNISTIdxGz(d / te_i, d / te_l)\n",
    "        cap = int(CFG.max_test)\n",
    "\n",
    "    seen = 0\n",
    "    for img, lab in ds:\n",
    "        if lab not in mapping.label_to_class:\n",
    "            continue\n",
    "        if CFG.canonicalize:\n",
    "            img = canonicalize_upright(img, grid=GRID)\n",
    "        y = int(mapping.label_to_class[lab])\n",
    "        yield img, y\n",
    "        seen += 1\n",
    "        if cap and seen >= cap:\n",
    "            break\n",
    "\n",
    "# Expose globals expected by Cell 3\n",
    "mapping_file, *_rest = split_files(CFG.split)\n",
    "mapping = parse_emnist_mapping_txt(Path(DATA_DIR) / mapping_file)\n",
    "n_classes = len(mapping.class_to_label)\n",
    "\n",
    "train_ds = _IterableFromFactory(lambda: iter_raw_images_labels(train=True))\n",
    "test_ds  = _IterableFromFactory(lambda: iter_raw_images_labels(train=False))\n",
    "\n",
    "phase_print(\"datasets_ready\", {\n",
    "    \"ok\": True,\n",
    "    \"split\": CFG.split,\n",
    "    \"mapping_file\": mapping_file,\n",
    "    \"n_classes\": n_classes,\n",
    "    \"canonicalize\": \"upright\" if CFG.canonicalize else \"raw\",\n",
    "    \"bin_threshold\": int(CFG.bin_threshold),\n",
    "    \"caps\": {\"max_train\": int(CFG.max_train), \"max_test\": int(CFG.max_test)},\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a36aca-11ad-4e83-8e39-50ba904f4f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Discrete margin repair\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os, time, json, math, hashlib\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Iterable, Iterator, Any, Optional\n",
    "\n",
    "# ============================================================\n",
    "# Helpers (stable hashing / I/O)\n",
    "# ============================================================\n",
    "\n",
    "def _now_utc_iso() -> str:\n",
    "    return datetime.now(timezone.utc).replace(microsecond=0).isoformat()\n",
    "\n",
    "def _sha256_hex(b: bytes) -> str:\n",
    "    return hashlib.sha256(b).hexdigest()\n",
    "\n",
    "def _stable_json(obj: Any) -> str:\n",
    "    # Deterministic JSON for hashing. Keep allow_nan=False.\n",
    "    return json.dumps(obj, sort_keys=True, ensure_ascii=False, separators=(\",\", \":\"), allow_nan=False)\n",
    "\n",
    "def _hash_obj(obj: Any) -> str:\n",
    "    return _sha256_hex(_stable_json(obj).encode(\"utf-8\"))\n",
    "\n",
    "def _sha256_file(path: Path, *, chunk: int = 1 << 20) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def _env_flag(name: str, default: str = \"0\") -> bool:\n",
    "    v = os.environ.get(name, default).strip().lower()\n",
    "    return v in {\"1\", \"true\", \"yes\", \"y\", \"on\"}\n",
    "\n",
    "def _argmax_stable(scores: List[int]) -> int:\n",
    "    # stable first-maximum (least index among maxima)\n",
    "    best_i = 0\n",
    "    best_v = scores[0]\n",
    "    for i in range(1, len(scores)):\n",
    "        v = scores[i]\n",
    "        if v > best_v:\n",
    "            best_v = v\n",
    "            best_i = i\n",
    "    return best_i\n",
    "\n",
    "def _best_competitor_stable(scores: List[int], y: int) -> Tuple[int, int]:\n",
    "    best_c = -1\n",
    "    best_v: Optional[int] = None\n",
    "    for c, v in enumerate(scores):\n",
    "        if c == y:\n",
    "            continue\n",
    "        if best_v is None or v > best_v:\n",
    "            best_c = c\n",
    "            best_v = v\n",
    "    return best_c, int(best_v if best_v is not None else 0)\n",
    "\n",
    "def _iter_ds(ds: Iterable[Tuple[bytes, int]]) -> Iterator[Tuple[bytes, int]]:\n",
    "    for x in ds:\n",
    "        yield x\n",
    "\n",
    "# ============================================================\n",
    "# Semantics Lock (explicit, hashed)\n",
    "# ============================================================\n",
    "\n",
    "SEM_LOCK = {\n",
    "    \"grid\": int(GRID),\n",
    "    \"n_pixels\": int(N_PIXELS),\n",
    "    \"canonicalize\": \"upright\" if bool(CFG.canonicalize) else \"raw\",\n",
    "    \"binarize_predicate\": \"v > bin_threshold\",\n",
    "    \"bin_threshold\": int(CFG.bin_threshold),\n",
    "    \"feature_schema\": \"unary: bias + active pixels\",\n",
    "    \"bias_feature\": \"bias:1\",\n",
    "    \"active_pixel_feature\": \"px:<idx>\",\n",
    "    \"cap_policy\": \"if cap_unary>0 and active>cap_unary: take top by (value desc, idx asc)\",\n",
    "    \"ordering_policy\": \"facts list begins with bias; then pixels in idx ascending (after optional cap)\",\n",
    "    \"dedup_policy\": \"preserve first occurrence\",\n",
    "    \"argmax_tie_break\": \"stable first-maximum (least index among maxima)\",\n",
    "}\n",
    "LOCK_HASH = _hash_obj(SEM_LOCK)\n",
    "\n",
    "# ============================================================\n",
    "# Dataset provenance (no dataset upload; optional strong hashes)\n",
    "# ============================================================\n",
    "\n",
    "# These names are created in Cell 2.\n",
    "# mapping_file exists; split_files exists. DATA_DIR exists from Cell 1.\n",
    "mapping_file, tr_i, tr_l, te_i, te_l = split_files(CFG.split)\n",
    "DATA_DIR_P = Path(DATA_DIR)\n",
    "\n",
    "paths = {\n",
    "    \"mapping_txt\": (DATA_DIR_P / mapping_file).resolve(),\n",
    "    \"train_images_gz\": (DATA_DIR_P / tr_i).resolve(),\n",
    "    \"train_labels_gz\": (DATA_DIR_P / tr_l).resolve(),\n",
    "    \"test_images_gz\": (DATA_DIR_P / te_i).resolve(),\n",
    "    \"test_labels_gz\": (DATA_DIR_P / te_l).resolve(),\n",
    "}\n",
    "\n",
    "for k, p in paths.items():\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"missing required EMNIST file for {k}: {p}\")\n",
    "\n",
    "strong_hash = _env_flag(\"HASH_DATASET\", \"0\")  # opt-in; may be slow\n",
    "prov = {\n",
    "    \"split\": str(CFG.split),\n",
    "    \"data_dir\": str(DATA_DIR_P),\n",
    "    \"files\": {\n",
    "        k: {\n",
    "            \"name\": p.name,\n",
    "            \"bytes\": int(p.stat().st_size),\n",
    "        }\n",
    "        for k, p in paths.items()\n",
    "    },\n",
    "    \"mapping_sha256\": _sha256_file(paths[\"mapping_txt\"]),\n",
    "    \"strong_hashes_included\": bool(strong_hash),\n",
    "}\n",
    "if strong_hash:\n",
    "    prov[\"files\"][\"train_images_gz\"][\"sha256\"] = _sha256_file(paths[\"train_images_gz\"])\n",
    "    prov[\"files\"][\"train_labels_gz\"][\"sha256\"] = _sha256_file(paths[\"train_labels_gz\"])\n",
    "    prov[\"files\"][\"test_images_gz\"][\"sha256\"]  = _sha256_file(paths[\"test_images_gz\"])\n",
    "    prov[\"files\"][\"test_labels_gz\"][\"sha256\"]  = _sha256_file(paths[\"test_labels_gz\"])\n",
    "\n",
    "PROV_HASH = _hash_obj(prov)\n",
    "\n",
    "# ============================================================\n",
    "# Unary facts from raw bytes (0..255) under Semantics Lock\n",
    "# Represent facts as strings to avoid repr()-based instability.\n",
    "# ============================================================\n",
    "\n",
    "def _feat_bias() -> str:\n",
    "    return \"bias:1\"\n",
    "\n",
    "def _feat_px(idx: int) -> str:\n",
    "    return f\"px:{int(idx)}\"\n",
    "\n",
    "def facts_unary_bytes(img: bytes, *, bin_threshold: int, cap_unary: int) -> List[str]:\n",
    "    if len(img) != N_PIXELS:\n",
    "        raise ValueError(f\"expected image length {N_PIXELS}, got {len(img)}\")\n",
    "\n",
    "    active: List[Tuple[int, int]] = []  # (value, idx)\n",
    "    thr = int(bin_threshold)\n",
    "    for i, v in enumerate(img):\n",
    "        if v > thr:\n",
    "            active.append((int(v), int(i)))\n",
    "\n",
    "    # forced bias; if no actives, bias-only\n",
    "    if not active:\n",
    "        return [_feat_bias()]\n",
    "\n",
    "    # deterministic cap (intensity desc then idx asc)\n",
    "    if cap_unary and cap_unary > 0 and len(active) > cap_unary:\n",
    "        active.sort(key=lambda t: (-t[0], t[1]))\n",
    "        active = active[:cap_unary]\n",
    "        # within cap, we still emit in idx order for stability/readability\n",
    "        active.sort(key=lambda t: (t[1],))\n",
    "    else:\n",
    "        active.sort(key=lambda t: (t[1],))\n",
    "\n",
    "    feats = [_feat_bias()] + [_feat_px(idx) for (_v, idx) in active]\n",
    "\n",
    "    # deterministic dedup preserve first occurrence (bias is unique anyway)\n",
    "    seen = set()\n",
    "    out: List[str] = []\n",
    "    for f in feats:\n",
    "        if f not in seen:\n",
    "            out.append(f)\n",
    "            seen.add(f)\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# Build finite training table + balanced probe set\n",
    "# ============================================================\n",
    "\n",
    "finite_imgs: List[bytes] = []\n",
    "finite_labels: List[int] = []\n",
    "finite_srcpos: List[int] = []\n",
    "\n",
    "for srcpos, (img, y) in enumerate(_iter_ds(train_ds)):\n",
    "    finite_imgs.append(img)\n",
    "    finite_labels.append(int(y))\n",
    "    finite_srcpos.append(int(srcpos))\n",
    "    if len(finite_imgs) >= int(CFG.finite_n):\n",
    "        break\n",
    "\n",
    "if len(finite_imgs) < int(CFG.finite_n):\n",
    "    raise RuntimeError(f\"train_ds yielded only {len(finite_imgs)} items; need finite_n={CFG.finite_n}.\")\n",
    "\n",
    "K = int(globals().get(\"n_classes\", max(finite_labels) + 1))\n",
    "\n",
    "probe_items: List[Tuple[bytes, int]] = []\n",
    "need = Counter({c: int(CFG.probe_per_class) for c in range(K)})\n",
    "for img, y in _iter_ds(test_ds):\n",
    "    y = int(y)\n",
    "    if 0 <= y < K and need[y] > 0:\n",
    "        probe_items.append((img, y))\n",
    "        need[y] -= 1\n",
    "        if all(v == 0 for v in need.values()):\n",
    "            break\n",
    "\n",
    "# ============================================================\n",
    "# Build feature table + postings\n",
    "# ============================================================\n",
    "\n",
    "FI: List[List[str]] = []\n",
    "TAB_post: Dict[str, List[int]] = defaultdict(list)\n",
    "\n",
    "feat_vocab: Dict[str, int] = {}  # stable IDs for bookkeeping only\n",
    "def _feat_id(f: str) -> int:\n",
    "    if f in feat_vocab:\n",
    "        return feat_vocab[f]\n",
    "    feat_vocab[f] = len(feat_vocab) + 1\n",
    "    return feat_vocab[f]\n",
    "\n",
    "for i in range(int(CFG.finite_n)):\n",
    "    feats = facts_unary_bytes(\n",
    "        finite_imgs[i],\n",
    "        bin_threshold=int(CFG.bin_threshold),\n",
    "        cap_unary=int(CFG.cap_unary),\n",
    "    )\n",
    "    FI.append(feats)\n",
    "    for f in feats:\n",
    "        TAB_post[f].append(i)\n",
    "        _feat_id(f)\n",
    "\n",
    "per_class = Counter(finite_labels)\n",
    "\n",
    "# Fingerprint of FI itself (small for finite_n=256; this is the real feature-table identity)\n",
    "FI_hash = _hash_obj({\n",
    "    \"finite_n\": int(CFG.finite_n),\n",
    "    \"facts\": FI,\n",
    "})\n",
    "\n",
    "table_fp = _hash_obj({\n",
    "    \"feat_vocab_size\": int(len(feat_vocab)),\n",
    "    \"postings_size\": int(len(TAB_post)),\n",
    "    \"postings_hist_prefix\": sorted([len(v) for v in TAB_post.values()])[:500],\n",
    "    \"fi_hash\": FI_hash,\n",
    "})\n",
    "\n",
    "# Dataset identity for *this* finite witness table (reconstructible from EMNIST + srcpos + lock)\n",
    "finite_table_id = _hash_obj({\n",
    "    \"prov_hash\": PROV_HASH,\n",
    "    \"lock_hash\": LOCK_HASH,\n",
    "    \"finite_srcpos\": finite_srcpos,\n",
    "    \"finite_labels\": finite_labels,  # redundant but practical\n",
    "    \"fi_hash\": FI_hash,              # binds extraction result\n",
    "})\n",
    "\n",
    "# ============================================================\n",
    "# Initialize integer weights and cached scores\n",
    "# ============================================================\n",
    "\n",
    "W: Dict[Tuple[int, str], int] = {}           # (class, feature) -> weight\n",
    "S: List[List[int]] = [[0] * K for _ in range(int(CFG.finite_n))]  # cached scores\n",
    "\n",
    "def _count_violations_from_cache(Sm: List[List[int]], Y: List[int], margin: int) -> Tuple[int, int]:\n",
    "    viol_pairs = 0\n",
    "    max_slack = 0\n",
    "    for i in range(len(Y)):\n",
    "        y = Y[i]\n",
    "        sy = Sm[i][y]\n",
    "        for c, sc in enumerate(Sm[i]):\n",
    "            if c == y:\n",
    "                continue\n",
    "            slack = (sc + margin) - sy\n",
    "            if slack > 0:\n",
    "                viol_pairs += 1\n",
    "                if slack > max_slack:\n",
    "                    max_slack = slack\n",
    "    return viol_pairs, max_slack\n",
    "\n",
    "def _predict_from_weights(img: bytes) -> Tuple[int, List[int]]:\n",
    "    feats = facts_unary_bytes(img, bin_threshold=int(CFG.bin_threshold), cap_unary=int(CFG.cap_unary))\n",
    "    scores = [0] * K\n",
    "    for f in feats:\n",
    "        for c in range(K):\n",
    "            scores[c] += W.get((c, f), 0)\n",
    "    pred = _argmax_stable(scores)\n",
    "    return pred, scores\n",
    "\n",
    "def _accuracy(items: List[Tuple[bytes, int]]) -> float:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for img, y in items:\n",
    "        pred, _ = _predict_from_weights(img)\n",
    "        correct += int(pred == int(y))\n",
    "        total += 1\n",
    "    return correct / total if total else float(\"nan\")\n",
    "\n",
    "# ============================================================\n",
    "# Training loop (compiled discrete margin repair)\n",
    "# ============================================================\n",
    "\n",
    "epoch_rows: List[dict] = []\n",
    "best = {\"epoch\": None, \"probe_acc\": float(\"-inf\"), \"weights_hash\": None}\n",
    "best_W: Optional[Dict[Tuple[int, str], int]] = None\n",
    "\n",
    "updates_total = 0\n",
    "\n",
    "for epoch in range(1, int(CFG.max_epochs) + 1):\n",
    "    ep_start = time.time()\n",
    "    viol_before, _ = _count_violations_from_cache(S, finite_labels, int(CFG.margin))\n",
    "\n",
    "    ep_updates = 0\n",
    "    ep_max_slack_seen = 0\n",
    "\n",
    "    for i in range(int(CFG.finite_n)):\n",
    "        y = finite_labels[i]\n",
    "        scores_i = S[i]\n",
    "\n",
    "        c_star, sc = _best_competitor_stable(scores_i, y)\n",
    "        sy = scores_i[y]\n",
    "        slack = (sc + int(CFG.margin)) - sy\n",
    "        if slack <= 0:\n",
    "            continue\n",
    "\n",
    "        feats = FI[i]\n",
    "        denom = max(1, len(feats))\n",
    "        steps = int(math.ceil(slack / denom))\n",
    "        if steps <= 0:\n",
    "            steps = 1\n",
    "        steps += int(CFG.overshoot_delta)\n",
    "\n",
    "        if slack > ep_max_slack_seen:\n",
    "            ep_max_slack_seen = int(slack)\n",
    "\n",
    "        # paired weight + cache updates, propagated through postings (compiled)\n",
    "        for f in feats:\n",
    "            W[(y, f)] = int(W.get((y, f), 0) + steps)\n",
    "            W[(c_star, f)] = int(W.get((c_star, f), 0) - steps)\n",
    "            for j in TAB_post[f]:\n",
    "                S[j][y] += steps\n",
    "                S[j][c_star] -= steps\n",
    "\n",
    "        ep_updates += 1\n",
    "        updates_total += 1\n",
    "        if updates_total >= int(CFG.max_updates):\n",
    "            break\n",
    "\n",
    "    viol_after, _ = _count_violations_from_cache(S, finite_labels, int(CFG.margin))\n",
    "    probe_acc = _accuracy(probe_items)\n",
    "\n",
    "    if probe_acc > float(best[\"probe_acc\"]):\n",
    "        best = {\"epoch\": int(epoch), \"probe_acc\": float(probe_acc), \"weights_hash\": None}\n",
    "        best_W = dict(W)\n",
    "\n",
    "    epoch_rows.append({\n",
    "        \"epoch\": int(epoch),\n",
    "        \"updates\": int(ep_updates),\n",
    "        \"viol_before\": int(viol_before),\n",
    "        \"viol_after\": int(viol_after),\n",
    "        \"max_slack_seen\": int(ep_max_slack_seen),\n",
    "        \"probe_acc\": float(probe_acc),\n",
    "        \"seconds\": float(time.time() - ep_start),  # diagnostics only (non-certificate)\n",
    "        \"w_keys\": int(sum(1 for v in W.values() if v != 0)),\n",
    "    })\n",
    "\n",
    "    if updates_total >= int(CFG.max_updates):\n",
    "        break\n",
    "    if viol_after == 0:\n",
    "        break\n",
    "\n",
    "def _hash_weights(Wd: Dict[Tuple[int, str], int]) -> str:\n",
    "    items: List[Tuple[int, str, int]] = []\n",
    "    for (c, f), w in Wd.items():\n",
    "        if w != 0:\n",
    "            items.append((int(c), str(f), int(w)))\n",
    "    items.sort()\n",
    "    return _sha256_hex(_stable_json(items).encode(\"utf-8\"))\n",
    "\n",
    "best[\"weights_hash\"] = _hash_weights(best_W if best_W is not None else W)\n",
    "weights_hash = _hash_weights(W)\n",
    "\n",
    "# ============================================================\n",
    "# Exact verification (definitional recomputation on finite table)\n",
    "# ============================================================\n",
    "\n",
    "def _finite_margin_check() -> dict:\n",
    "    violations = 0\n",
    "    prefix = []\n",
    "    margin = int(CFG.margin)\n",
    "\n",
    "    for i, feats in enumerate(FI):\n",
    "        y = finite_labels[i]\n",
    "        scores = [0] * K\n",
    "        for f in feats:\n",
    "            for c in range(K):\n",
    "                scores[c] += W.get((c, f), 0)\n",
    "        sy = scores[y]\n",
    "        for c, sc in enumerate(scores):\n",
    "            if c == y:\n",
    "                continue\n",
    "            slack = (sc + margin) - sy\n",
    "            if slack > 0:\n",
    "                violations += 1\n",
    "                if len(prefix) < 20:\n",
    "                    prefix.append({\n",
    "                        \"i\": int(i), \"y\": int(y), \"c\": int(c),\n",
    "                        \"sy\": int(sy), \"sc\": int(sc), \"slack\": int(slack),\n",
    "                    })\n",
    "\n",
    "    return {\n",
    "        \"ok\": (violations == 0),\n",
    "        \"violations_total\": int(violations),\n",
    "        \"violations_prefix\": prefix,\n",
    "    }\n",
    "\n",
    "def _cache_consistency_check() -> dict:\n",
    "    mismatches = 0\n",
    "    prefix = []\n",
    "    for i, feats in enumerate(FI):\n",
    "        for c in range(K):\n",
    "            s_def = 0\n",
    "            for f in feats:\n",
    "                s_def += W.get((c, f), 0)\n",
    "            if int(S[i][c]) != int(s_def):\n",
    "                mismatches += 1\n",
    "                if len(prefix) < 20:\n",
    "                    prefix.append({\n",
    "                        \"i\": int(i), \"c\": int(c),\n",
    "                        \"cache\": int(S[i][c]), \"def\": int(s_def),\n",
    "                    })\n",
    "    return {\n",
    "        \"ok\": (mismatches == 0),\n",
    "        \"mismatches_total\": int(mismatches),\n",
    "        \"mismatches_prefix\": prefix,\n",
    "    }\n",
    "\n",
    "FULL = _finite_margin_check()\n",
    "CACHE = _cache_consistency_check()\n",
    "OK = bool(FULL[\"ok\"] and CACHE[\"ok\"])\n",
    "\n",
    "# ============================================================\n",
    "# Deterministic certificate payload vs run record\n",
    "# ============================================================\n",
    "\n",
    "cfg_hash = _hash_obj({\n",
    "    \"seed\": int(CFG.seed),\n",
    "    \"run_mode\": str(CFG.run_mode),\n",
    "    \"split\": str(CFG.split),\n",
    "    \"canonicalize\": bool(CFG.canonicalize),\n",
    "    \"bin_threshold\": int(CFG.bin_threshold),\n",
    "    \"finite_n\": int(CFG.finite_n),\n",
    "    \"cap_unary\": int(CFG.cap_unary),\n",
    "    \"margin\": int(CFG.margin),\n",
    "    \"overshoot_delta\": int(CFG.overshoot_delta),\n",
    "    \"max_epochs\": int(CFG.max_epochs),\n",
    "    \"max_updates\": int(CFG.max_updates),\n",
    "    \"probe_per_class\": int(CFG.probe_per_class),\n",
    "})\n",
    "\n",
    "CERT = {\n",
    "    \"schema\": \"typedrepair: compiled discrete margin repair (integer weights; postings + cache)\",\n",
    "    \"ok\": bool(OK),\n",
    "    \"lock\": SEM_LOCK,\n",
    "    \"lock_hash\": LOCK_HASH,\n",
    "    \"cfg_hash\": cfg_hash,\n",
    "    \"dataset_provenance\": prov,          # deterministic; strong hashes optional but still deterministic\n",
    "    \"prov_hash\": PROV_HASH,\n",
    "    \"finite_table\": {\n",
    "        \"finite_table_id\": finite_table_id,\n",
    "        \"finite_n\": int(CFG.finite_n),\n",
    "        \"src_positions_in_train_stream\": list(map(int, finite_srcpos)),\n",
    "        \"labels\": list(map(int, finite_labels)),\n",
    "        \"class_hist\": dict(sorted({int(k): int(v) for k, v in per_class.items()}.items())),\n",
    "        \"fi_hash\": FI_hash,\n",
    "        \"table_fingerprint\": table_fp,\n",
    "    },\n",
    "    \"features\": {\n",
    "        \"schema\": \"bias:1 + px:<idx> for pixels with value > bin_threshold; optional cap by intensity\",\n",
    "        \"cap_unary\": int(CFG.cap_unary),\n",
    "        \"feat_vocab_size\": int(len(feat_vocab)),\n",
    "        \"postings_size\": int(len(TAB_post)),\n",
    "    },\n",
    "    \"weights\": {\n",
    "        \"weights_hash\": weights_hash,\n",
    "        \"nonzero_keys\": int(sum(1 for v in W.values() if v != 0)),\n",
    "    },\n",
    "    \"verification\": {\n",
    "        \"finite_full_margin_check\": FULL,\n",
    "        \"cache_consistency_check\": CACHE,\n",
    "        \"note\": \"Exact audit on finite witness table; cache check enforces definitional score equality on the table.\",\n",
    "    },\n",
    "}\n",
    "\n",
    "CERT_HASH = _hash_obj(CERT)\n",
    "\n",
    "# Non-certificate run record (may include time, probe curve, etc.)\n",
    "RUN_RECORD = {\n",
    "    \"experiment\": \"logical_backpropagation_compiled_discrete_margin_repair_curve\",\n",
    "    \"created_utc\": _now_utc_iso(),\n",
    "    \"emnist\": {\n",
    "        \"split\": str(CFG.split),\n",
    "        \"n_classes\": int(K),\n",
    "        \"mapping_file\": str(mapping_file),\n",
    "        \"canonicalize\": \"upright\" if bool(CFG.canonicalize) else \"raw\",\n",
    "        \"bin_threshold\": int(CFG.bin_threshold),\n",
    "    },\n",
    "    \"cfg\": {\n",
    "        \"finite_n\": int(CFG.finite_n),\n",
    "        \"cap_unary\": int(CFG.cap_unary),\n",
    "        \"margin\": int(CFG.margin),\n",
    "        \"overshoot_delta\": int(CFG.overshoot_delta),\n",
    "        \"max_epochs\": int(CFG.max_epochs),\n",
    "        \"max_updates\": int(CFG.max_updates),\n",
    "        \"probe_per_class\": int(CFG.probe_per_class),\n",
    "        \"seed\": int(CFG.seed),\n",
    "    },\n",
    "    \"epoch_curve\": epoch_rows,  # includes seconds (non-deterministic) by design\n",
    "    \"best_probe\": dict(best),   # diagnostic only (not certified)\n",
    "}\n",
    "\n",
    "PAYLOAD = {\n",
    "    \"certificate\": dict(CERT, certificate_hash=CERT_HASH),\n",
    "    \"run_record\": RUN_RECORD,\n",
    "}\n",
    "\n",
    "# For phase logging / later cells\n",
    "PHASES[\"logical_backpropagation_compiled_curve\"] = PAYLOAD\n",
    "\n",
    "# ============================================================\n",
    "# Compact summary\n",
    "# ============================================================\n",
    "\n",
    "final_v = epoch_rows[-1][\"viol_after\"] if epoch_rows else None\n",
    "max_v = int(CFG.finite_n) * (int(K) - 1)\n",
    "\n",
    "print(\"\\nSummary (certificate-relevant):\")\n",
    "print(f\"  EMNIST/{CFG.split} | finite_n={CFG.finite_n} | classes={K} | margin={CFG.margin} | cap_unary={CFG.cap_unary}\")\n",
    "print(f\"  Violations: {final_v}/{max_v} (finite-table) | Cache: {'OK' if CACHE.get('ok') else 'FAIL'} | Finite-check: {'OK' if FULL.get('ok') else 'FAIL'}\")\n",
    "print(f\"  Weights hash: {weights_hash}\")\n",
    "print(f\"  Finite table id: {finite_table_id}\")\n",
    "print(f\"  Provenance hash: {PROV_HASH}  (HASH_DATASET={'1' if strong_hash else '0'})\")\n",
    "print(f\"  Lock hash: {LOCK_HASH}\")\n",
    "print(f\"  CERTIFICATE HASH (citable): {CERT_HASH}\")\n",
    "\n",
    "# Optional diagnostic print\n",
    "best_epoch = best.get(\"epoch\")\n",
    "if best_epoch is not None:\n",
    "    print(\"\\nDiagnostics (non-certified):\")\n",
    "    print(f\"  Best probe: epoch {best_epoch} | acc={best.get('probe_acc', float('nan')):.3f} | weights_hash(best)={best.get('weights_hash')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd82b3a-d3b7-44c1-af23-536bf18dca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "def stable_json_bytes(obj) -> bytes:\n",
    "    return json.dumps(obj, sort_keys=True, separators=(\",\", \":\"), ensure_ascii=False).encode(\"utf-8\")\n",
    "\n",
    "def emit_artifact(record: dict, out_dir: Path) -> Path:\n",
    "    out_dir = out_dir.resolve()\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    payload = stable_json_bytes(record)\n",
    "    h = hashlib.sha256(payload).hexdigest()\n",
    "\n",
    "    # Deterministic filename, easy to locate\n",
    "    out_path = out_dir / f\"artifact_{h}.json\"\n",
    "    out_path.write_bytes(payload)\n",
    "\n",
    "    # Optional: write a stable “latest” pointer for convenience\n",
    "    (out_dir / \"LATEST.txt\").write_text(str(out_path.name) + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "    # Hard assertions: fail loudly instead of “printing a hash and moving on”\n",
    "    assert out_path.exists(), f\"Artifact write failed: {out_path}\"\n",
    "    assert out_path.stat().st_size == len(payload), \"Short write detected.\"\n",
    "\n",
    "    print(\"Artifact SHA-256:\", h)\n",
    "    print(\"Artifact path:\", str(out_path))\n",
    "    return out_path\n",
    "\n",
    "# Example: build the record you are already hashing/printing\n",
    "certificate_record = {\n",
    "    \"created_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "    \"lock\": {\n",
    "        \"bin_threshold\": 127,\n",
    "        \"cap_unary\": 128,\n",
    "        \"finite_n\": 256,\n",
    "        # include your dataset/schema fingerprints here\n",
    "    },\n",
    "    \"result\": {\n",
    "        \"violations\": 0,\n",
    "        \"cache_ok\": True,\n",
    "        \"probe_acc\": 0.710,\n",
    "        \"epoch_best\": 6,\n",
    "    },\n",
    "    # include weights + any other structures you certify (prefer a compressed encoding if large)\n",
    "    # \"weights\": ...\n",
    "}\n",
    "\n",
    "emit_artifact(certificate_record, out_dir=Path(\"artifacts\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
